

[
  
  
    
    
      {
        "title": "Hello World",
        "excerpt": "My first post! I’m primarily a developer that works with UIKit in iOS and I would love to bring some of those elements into the 3D world that ARKit and SceneKit live in. Luckily, it’s not that hard! Introducing: UIViewControllerNode. It’s a pretty simple concept, it’s just a SCNNode subclass with a SCNPlane geometry that is textured with the UIViewController’s view.\n\nSee it on Github  github\n\n\nA few extra bits:\n\n  I added the option to have it billboarded so that the view is always visible\n  The viewController parameter in the init is there so the node will retain the viewController for you. As long as you add it as a child of something or retain it in some other way, the viewController sticks around too\n\n\nI’m pretty surprised on how interactions work out of the box, you can scroll the tableView in my sample app, even if there are multiple of them. Sometimes it’s a bit hard to scroll, I wonder if it’s my UITapGestureRecognizer interfering with things or something is up with the hit-testing\nI have a bunch of ideas on how to use this that I’ll be exploring in future posts. In the meantime, feel free to use it yourself!\n",
        "content": "My first post! I’m primarily a developer that works with UIKit in iOS and I would love to bring some of those elements into the 3D world that ARKit and SceneKit live in. Luckily, it’s not that hard! Introducing: UIViewControllerNode. It’s a pretty simple concept, it’s just a SCNNode subclass with a SCNPlane geometry that is textured with the UIViewController’s view.\n\nSee it on Github  github\n\n\nA few extra bits:\n\n  I added the option to have it billboarded so that the view is always visible\n  The viewController parameter in the init is there so the node will retain the viewController for you. As long as you add it as a child of something or retain it in some other way, the viewController sticks around too\n\n\nI’m pretty surprised on how interactions work out of the box, you can scroll the tableView in my sample app, even if there are multiple of them. Sometimes it’s a bit hard to scroll, I wonder if it’s my UITapGestureRecognizer interfering with things or something is up with the hit-testing\nI have a bunch of ideas on how to use this that I’ll be exploring in future posts. In the meantime, feel free to use it yourself!\n",
        "url": "/tools/2019/02/05/hello-world/"
      },
    
      {
        "title": "A Carousel of Textures",
        "excerpt": "While working on other things, I discovered how performant it was to switch textures on an SCNNode. What if I could switch textures quickly, like at the speed of scroll?\n",
        "content": "While working on other things, I discovered how performant it was to switch textures on an SCNNode. What if I could switch textures quickly, like at the speed of scroll? And thus, ARPKCarousel was born. \nspoiler: it’s plenty fast in a scrollview\n\n\n\nSee it on Github  github\n\n\nI like working with usdz files so I grabbed one off Apple’s QuickLook Gallery and chucked it into a SCNView. Then underneath, I have a UICollectionView and the gist of it is whatever is at index 0 is the texture of the model. I added a few quality of life tweaks:\n\n  the SCNView is placed to hide the ‘active’ texture cell but not the label so you can get some context\n  The UICollectionView has a large right contentInset to make sure the last cell can fit underneath slot 0\n  I used a subclass of UICollectionViewFlowLayout to get the cells to snap to position 0 nicely (thanks to this SO post https://stackoverflow.com/a/49617263)\n\n\nThere could be a lot of other cool things to make this more useable but I’d like to move onto experiments on how this works in AR in future posts\n",
        "url": "/experiments/2019/02/13/3D-carousel/"
      },
    
      {
        "title": "A Failed Experiment with a Carousel",
        "excerpt": "When trying to make an inline configuration carousel, I realized while it’s a great idea in a UIView, it’s a dumb idea in AR\n",
        "content": "When trying to make an inline configuration carousel, I realized while it’s a great idea in a UIView, it’s a dumb idea in AR.\n\nSo I was trying to recreate the experience in my previous post in AR. I drew a crappy picture to show what I was trying to accomplish\n\n  \n  \n\n\nI realized that using the chair in the example would be very difficult as it’s a big chair and where would I put the carousel? That should have given me a clue, ugh. I swapped out the model with a vase given in Apple’s Quick Look gallery to try and make it a bit more manageable. I even implemented a custom UICollectionViewLayout to shrink non-focused cells (seen in the video below).\n\n\n\nAlas, as I struggled with it, I eventually came to the conclusion that this wasn’t going to work. Even with the shrinking cells, the carousel would need to be huge to fit the model. This is the vase on my desk in AR\n\n  \n  \n\n\nIn AR, realistic models like the chair and vase need to be real-world sizes and especially with a limited window of a phone screen, you can’t expect a user to wave their phone around to see that a model has a carousel to change its color. That field of view will get bigger with AR glasses/goggles but even then, what if you were looking at a car? (Excuse my terrible Photoshop skills)\n\n  \n  \n\n\nYour field of view would be about the green rectangle and the carousel could span the red rectangle on the ground. You can’t be expected to swivel your head to see all the options in the carousel, configurations like this should be easy to view so you can choose your option quickly\n\nI would still like to make a color picker ‘attached’ to the model in 3D space instead of a 2D collectionView stuck in a corner. Hopefully I can get it right in the next post\n",
        "url": "/experiments/2019/03/18/failed-carousel-test/"
      },
    
      {
        "title": "AR Translation Proof of Concept",
        "excerpt": "I made a proof of concept of live translating next to your face in AR\n",
        "content": "Image credit Google\n\nIn the Google I/O conference last month, Google finished off their keynote with some AR “magic”. They showed some AR glasses with the ability to allow people to communicate with each other through a language barrier.\n\nGoogle AR Glasses\n\nThe person would speak and some subtitles appear next to their head. The subtitles could be translated, allowing the wearer to understand the previously unintelligble moonspeak though the use of technology. I got excited about this, I don’t know any languages other than English, and suddenly technology is making up for my deficiencies. Then I had a think about it and realized, hey, this isn’t that special, we can do this on phones today.\n\n\n\nSo for this proof-of-concept, there are 3 main technologies in play: Speech Recognition, Translation, and Augmented Reality. I’ll give a bit of an overview and how they came together for this demo. The crazy thing is, this is all able to work straight on-device, I don’t need to send anything through the interwebs to be churned though by supercomputers, this is being done on a mobile chip.\n\nSpeech Recognition\n\n\nFor speech recognition, I’m using the same tech that powers Siri on the phone. Apple provide a framework called Speech, which has been available since iOS 10 but in iOS13, we have the ability to do it all on device. I’m surprised at how quick it is, but as you can see, it sometimes has hiccups. If you want to get in-depth on it, I modified some code from one of Apple’s sample apps here to do live transcribing of speech into text.\n\nThere are some smarts in this, you can see it trying to adjust the text as it thinks. But the smarts are limited, for example, it can’t transcribe grammar. I’m cheating a little in adding stops at the end of pauses, if speaking is paused for 2 seconds, it will auto-add a period because I’m making an assumption that people will generally spit out sentences at a time before pausing. In order to add grammatical smarts, we’d have to train and add a machine learning model to recognize sentences. I can only imagine how difficult it must be to add commas and question marks.\n\nTranslation\n\n\nSince iOS14, iOS has the ability to do translations on-device via their Translate app. Unfortunately they have not yet made this framework available to developers. However, Google provides this through their MLKit framework. I’m able to download the needed language files in the app (I skipped that part in the demo) and have them ready for quick translations. It’s pretty easy to use. In my demo, when it detects the 2-second pause and ends the sentence, I translate the subtitled string and add it to the log below.\n\nThe drawback is that translation is still a little wonky. There are still grammatical smarts that need to be figured out. Just try the Google Translate app and if you know the second language, you can see it’s a bit off. I tested it with a coworker who spoke Russian and she mentioned the words were out of order since it was translating the words literally. “I love you” becomes “I you love” in Russian for example. When I checked Korean translations with my mom, she was saying the translations oscillated between being really formal to being almost rude in their casual-ness.\n\nAugmented Reality\n\nThe juicy part of this stack! This was the most exciting for me. If I’m being perfectly honest though, it was probably the most unnecessary. The app probably works better if I just showed the text at the bottom of the screen. In fact, Apple are releasing an app later this year called Live Captions that subtitles spoken speech, it just shows the text at the top of the screen. This is one of the pitfalls of AR on the phone: does it really make a better experience than just shoving a label, map, 3D render, etc on good old 2D space? Which is why this is a proof-of-concept and not a real app I’m trying to release on the App Store.\n\nWith that out of the way, where this really shines is in context of Google’s presentation: if we had AR glasses on our faces. For this demo, I’m using an open-source project I made and had posted about previously UIViewControllerNode where I can make a standard UIView float in AR. In order to use this, I need to use ARKit+SceneKit vs the easier-to-use RealityKit because RealityKit does not give me the advanced material controls that SceneKit can. I’m using face detection and simply making the panel stick to floating a few centimeters to the side of my head, pretty simple!\n\n\n\nWhen  I turned my head, the panel was obscuring my face so I decided to try and add some occlusion to make the demo look a little better. If I had stayed with RealityKit, I would get this almost for free. RealityKit does lots of neat things with figuring out 3D meshes in the real world using either the FaceID sensors or the LiDAR sensor on the back and determining what goes in front of what. In ARKit-land, we have to do all that manually. In the demo, I followed a tip from Apple’s sample code to take the face mesh and render it before the panel. If I throw out the color it looks invisible but will block the panel if my face is in front. In the above screenshot, I turned the white color back on so you can see what the face mesh looks like. It isn’t full coverage, the face mesh is only in the shape of a mask, so in the video you can kind of see it slice through my face and behind my eyeballs if I turn my head a certain way. Creepy, huh?\n\nI’m really excited to see what new AR goodies comes in Apple’s WWDC conference next week. Maybe the Translations SDK will finally be made available. Maybe the long-awaited AR goggles will arrive? At the very least, I am hoping for some great enhancements to ARKit and RealityKit to continue the train of great AR content that we’ve seen in the last few months from Meta, Google, and Niantic.\n",
        "url": "/experiments/2022/05/31/Subtitle-experiment/"
      },
    
  
  
  
  {
    "title": "About this website",
    "excerpt": "\n",
    "content": "My name is Gerald and I’m an iOS developer based in Melbourne, Australia. I have an interest in Augmented Reality and like to experiment with the possibilities of it.\n\ntwitter\n \nlinkedin\n \ngithub\n\n",
    "url": "/about/"
  },
  
  {
    "title": "AR Playkit",
    "excerpt": "\n",
    "content": "Experimentations and tools for ARKit\n",
    "url": "/"
  }
  
]

